{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products = 10\n",
      "Number of flips = 10\n",
      "Value of kappa = 0.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym, recogym\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "pd.options.mode.chained_assignment = None \n",
    "# from inspect import getsource\n",
    "from recogym.evaluate_agent_sale import verify_agents_sale, display_metrics, verify_agents_sale_extended \n",
    "from tqdm import tqdm\n",
    "\n",
    "# env_1_sale_args is a dictionary of default parameters (i.e. number of products)\n",
    "from recogym import env_1_sale_args, Configuration\n",
    "from recogym.agents.sale_agent import train_agents, train_timeagents\n",
    "from recogym.envs.utils_sale import format_avg_result, avg_result, format_avg_result_extended, avg_result_extended\n",
    "\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_sale_args['random_seed'] = 0\n",
    "env_1_sale_args['num_products'] = 10\n",
    "env_1_sale_args['number_of_flips'] = 10 \n",
    "\n",
    "num_products = env_1_sale_args['num_products']\n",
    "print('Number of products =',num_products)\n",
    "print('Number of flips =',env_1_sale_args['number_of_flips'])\n",
    "nb_flips = env_1_sale_args['number_of_flips']\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_sale_args['random_seed'] = 42\n",
    "env_1_sale_args['mu_sale'] = False \n",
    "\n",
    "# env_1_sale_args['kappa'] = 0.5\n",
    "print('Value of kappa =',env_1_sale_args['kappa'])\n",
    "\n",
    "# Initialize the gym \n",
    "env = gym.make('reco-gym-sale-v1')\n",
    "env.init_gym(env_1_sale_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository to save pickles\n",
    "data_repo = 'data_conversion/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train baseline agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose number of users for training and AB test\n",
    "# Number of users for the training\n",
    "# env_1_sale_args['num_users'] = 6 ##tochange !!\n",
    "env_1_sale_args['num_users'] = 5000\n",
    "num_users = env_1_sale_args['num_users']\n",
    "\n",
    "# Number of users for the A/B test\n",
    "env_1_sale_args['num_users_AB'] = 7 ##tochange !!\n",
    "# env_1_sale_args['num_users_AB'] = 5000\n",
    "num_users_AB = env_1_sale_args['num_users_AB']\n",
    "\n",
    "# Choose user features\n",
    "from recogym.agents.sale_agent import CountViewsClicksFeatureProvider, CountViewsFeatureProvider, ShareViewsClicksFeatureProvider, ShareViewsFeatureProvider\n",
    "vc_feature = CountViewsClicksFeatureProvider(env.config)\n",
    "v_feature = CountViewsFeatureProvider(env.config)\n",
    "vc_share_feature = ShareViewsClicksFeatureProvider(env.config)\n",
    "v_share_feature = ShareViewsFeatureProvider(env.config)\n",
    "features = {'vc':vc_feature,\n",
    "           'v':v_feature,\n",
    "           'vc_share':vc_share_feature,\n",
    "           'v_share':v_share_feature}\n",
    "feature_name = 'v_share'\n",
    "feature = features[feature_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'> Number of A/B tests</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose number of A/B tests\n",
    "num_AB_tests = 2 ##tochange !\n",
    "# num_AB_tests = 25 ##tochange !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logs loaded---\n"
     ]
    }
   ],
   "source": [
    "agents={}\n",
    "logs={}\n",
    "\n",
    "############## Random agent\n",
    "name_agent = 'rand'+str(nb_flips)\n",
    "from recogym.agents import RandomAgent, random_args\n",
    "random_agent = RandomAgent(Configuration(random_args))\n",
    "agents[name_agent] = random_agent\n",
    "\n",
    "\n",
    "try:\n",
    "    logs[name_agent] = pkl.load(open(data_repo + 'data' + str(num_users) + name_agent + '.pkl','rb'))\n",
    "    print('--- Logs loaded---')\n",
    "except: \n",
    "    print(\"--- Generate logs ---\")\n",
    "    logs[name_agent] = deepcopy(env).generate_logs(num_users)\n",
    "    print(data_repo + 'data' + str(num_users) + name_agent + '.pkl')\n",
    "    pkl.dump(logs[name_agent], open(data_repo + 'data' + str(num_users) + name_agent + '.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from recogym.agents.sale_agent_repeated import PseudoRewards, ShareViewsCountClicksFeatureProvider\n",
    "data = logs[name_agent]\n",
    "RewardTilde = PseudoRewards(clicks_only=False)\n",
    "RewardTilde.observe(data)\n",
    "RewardTilde.pseudo_observe(RewardTilde.data_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6         0.0\n",
       "7         0.0\n",
       "8         0.0\n",
       "9         0.0\n",
       "10        0.0\n",
       "         ... \n",
       "511604    0.0\n",
       "511605    0.0\n",
       "511606    0.0\n",
       "511607    0.0\n",
       "511608    0.0\n",
       "Name: pseudo_y, Length: 392170, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RewardTilde.data_rewards[\"pseudo_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserFeatures = ShareViewsCountClicksFeatureProvider(config = env.config )\n",
    "UserFeatures.reset()\n",
    "# UserFeatures.observe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UserFeatures.features()\n",
    "UserFeatures.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_products': 10,\n",
       " 'num_users': 5000,\n",
       " 'random_seed': 42,\n",
       " 'prob_leave_bandit': 0.01,\n",
       " 'prob_leave_organic': 0.01,\n",
       " 'prob_bandit_to_organic': 0.05,\n",
       " 'prob_organic_to_bandit': 0.25,\n",
       " 'normalize_beta': False,\n",
       " 'with_ps_all': False,\n",
       " 'num_clusters': 2,\n",
       " 'phi_var': 0.1,\n",
       " 'K': 5,\n",
       " 'sigma_omega_initial': 1,\n",
       " 'sigma_omega': 0.1,\n",
       " 'number_of_flips': 10,\n",
       " 'sigma_mu_organic': 3,\n",
       " 'change_omega_for_bandits': False,\n",
       " 'kappa': 0.2,\n",
       " 'sigma_Lambda': 1,\n",
       " 'psale_scale': 0.15,\n",
       " 'delta_for_clicks': 0,\n",
       " 'delta_for_views': 0,\n",
       " 'sig_coef': 1,\n",
       " 'mu_sale': False,\n",
       " 'sigma_mu_sale': 0,\n",
       " 'num_users_AB': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_1_sale_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question : how to initialize Q0 ? Train a discounted agent as a start to avoid long nb of iterations ? Better to start bad to check that it does in the right direction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from pprint import pprint\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulator rewards, online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recogym.agents.sale_agent_reinforce import Model, make_seed, REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config :  {'learning_rate': 0.014, 'seed': 42, 'gamma': 0.98}\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.015\n",
    "gamma = 1.0  # every second counts the same\n",
    "seed = 1235\n",
    "\n",
    "config = {\n",
    "    'learning_rate': 0.014,\n",
    "    'seed': env_1_sale_args['random_seed'],\n",
    "    'gamma': .98\n",
    "}\n",
    "\n",
    "print(\"Config : \",config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = REINFORCE(config,env,UserFeatures)\n",
    "\n",
    "# agent.train(n_trajectories=50, n_update=5)\n",
    "\n",
    "# # Save the model\n",
    "# datetag = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "# PATH = f\"data/RL/reinforce_{datetag}.pth\"\n",
    "# print(PATH)\n",
    "# torch.save(agent.model.state_dict(), PATH)\n",
    "# # agent.model.state_dict = torch.load(PATH)\n",
    "# agent.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoREINFORCE(REINFORCE):\n",
    "        \n",
    "    def __init__(self, config, env, user_features, pseudo_reward_provider):\n",
    "        super().__init__(config, env, user_features)\n",
    "        self.pseudo_reward_provider = pseudo_reward_provider\n",
    "    \n",
    "    def _compute_pseudo_returns(self, rewards):\n",
    "        num_rew = len(rewards)\n",
    "        exponents = np.arange(num_rew)\n",
    "        gammas = np.power(self.gamma, exponents)\n",
    "        \n",
    "        return rewards.dot(gammas)\n",
    "    \n",
    "        \n",
    "    def optimize_model(self, n_trajectories):\n",
    "\n",
    "        reward_trajectories = np.empty(n_trajectories)\n",
    "        loss = 0.\n",
    "        \n",
    "        for i in range(n_trajectories):\n",
    "            print('trajectory n°'+str(i))\n",
    "            print(datetime.now())\n",
    "            traj_rewards = []  # rewards of the trajectory\n",
    "            traj_pseudo_rewards = []  # rewards of the trajectory\n",
    "            traj_proba = 0.  # sum of log-probabilities of trajectory\n",
    "            \n",
    "            # Build trajectory\n",
    "            done = False\n",
    "            self.env.reset()\n",
    "            # reset user features\n",
    "            self.reset()\n",
    "            obs_raw, _, done, reward = self.env.step(None)\n",
    "            log = self.observation_to_log(obs_raw,reward)\n",
    "            self.user_features.observe(log, memory=False)\n",
    "            state = self.user_features.features()\n",
    "            state = torch.from_numpy(state).float()\n",
    "            while not done:\n",
    "                action = self.model.select_action(state)  # can be cast to int for action idx\n",
    "                # Get proba\n",
    "                prob = self.model(state)[int(action)]\n",
    "                traj_proba += torch.log(prob)\n",
    "                \n",
    "                obs_raw, reward, done, info = self.env.step(int(action))\n",
    "                log = self.observation_to_log(obs_raw,reward)\n",
    "                self.user_features.observe(log, memory=False)\n",
    "                state = self.user_features.features()\n",
    "                state = torch.from_numpy(state).float()\n",
    "\n",
    "                self.pseudo_reward_provider.pseudo_observe(log)\n",
    "                pseudo_reward = self.pseudo_reward_provider.data_rewards\n",
    "                if reward > 0:\n",
    "                    print(\"reward\", reward)\n",
    "                    print(\"pseudo rewards\", pseudo_reward)\n",
    "                # Store the new reward\n",
    "                traj_rewards.append(reward)\n",
    "                \n",
    "                \n",
    "            traj_rewards = np.array(traj_rewards)  # NumPy array\n",
    "            \n",
    "            # Get total reward\n",
    "            total_reward = self._compute_returns(traj_rewards)  # NumPy array\n",
    "            reward_trajectories[i] = total_reward\n",
    "            \n",
    "            loss = loss + total_reward * traj_proba / n_trajectories  # accumulate the negative criterion\n",
    "            # reset user features construction and delete logs in memory\n",
    "            self.reset()\n",
    "            \n",
    "        self.env.close()  # important\n",
    "        \n",
    "        loss = -loss\n",
    "        \n",
    "        # The following lines take care of the gradient descent step for the variable loss\n",
    "        # that you need to compute.\n",
    "        print(\"Loss:\", loss.data.numpy())\n",
    "        \n",
    "        # Discard previous gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        return reward_trajectories\n",
    "    \n",
    "    def train(self, n_trajectories, n_update):\n",
    "        \"\"\"Training method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expected gradient\n",
    "        n_update : int\n",
    "            The number of gradient updates\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        final_update = self.current_ep + n_update\n",
    "        rewards = self.rewards  # restart the reward record\n",
    "        for episode in range(self.current_ep, final_update):\n",
    "            rewards.append(self.optimize_model(n_trajectories))\n",
    "            print(f'Episode {episode + 1}/{final_update}: rewards ' \n",
    "                  +f'{round(rewards[-1].mean(), 2)} +/- {round(rewards[-1].std(), 2)}')\n",
    "            self.current_ep += 1\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards[i]) for i in range(len(rewards))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PseudoREINFORCE(config,env,UserFeatures,RewardTilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory n°0\n",
      "2020-09-30 17:27:04.002220\n",
      "trajectory n°1\n",
      "2020-09-30 17:27:05.221006\n",
      "trajectory n°2\n",
      "2020-09-30 17:27:05.599771\n",
      "Loss: -0.0\n",
      "Episode 1/2: rewards 0.0 +/- 0.0\n",
      "trajectory n°0\n",
      "2020-09-30 17:27:07.460067\n",
      "trajectory n°1\n",
      "2020-09-30 17:27:29.560966\n",
      "trajectory n°2\n",
      "2020-09-30 17:27:33.391382\n",
      "Loss: -0.0\n",
      "Episode 2/2: rewards 0.0 +/- 0.0\n",
      "data/RL/Pseudoreinforce_30092020-172802.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train(n_trajectories=3, n_update=2)\n",
    "\n",
    "# Save the model\n",
    "datetag = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "PATH = f\"data/RL/Pseudoreinforce_{datetag}.pth\"\n",
    "print(PATH)\n",
    "torch.save(agent.model.state_dict(), PATH)\n",
    "# agent.model.state_dict = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
